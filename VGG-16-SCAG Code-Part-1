{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2901,"status":"ok","timestamp":1667720959217,"user":{"displayName":"Manish Sahoo","userId":"13850174497932657096"},"user_tz":-330},"id":"xrkTTKADkLsn","outputId":"291798f1-af67-47c8-ef8d-e4470d9fa565"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torchmetrics in /usr/local/lib/python3.7/dist-packages (0.10.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (4.1.1)\n","Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.12.1+cu113)\n","Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.21.6)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.9)\n"]}],"source":["!pip install torchmetrics"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2CVg3KwOCMdk"},"outputs":[],"source":["# IMPORTING ALL THE NECESSARY LIBRARIES.\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torch.optim as optim\n","from torch.utils.tensorboard import SummaryWriter\n","\n","import torchvision\n","import torchvision.transforms as transforms\n","from torchvision.utils import save_image\n","from torchmetrics.functional.classification import multiclass_f1_score, multiclass_jaccard_index \n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","import os\n","from sklearn.preprocessing import LabelEncoder\n","import cv2\n","import skimage.io as io\n","from skimage import util\n","from skimage import measure\n","from skimage import segmentation, morphology, color\n","import scipy.ndimage as ndi\n","from scipy.optimize import curve_fit\n","from PIL import Image\n","\n","from itertools import product\n","from collections import namedtuple\n","from collections import OrderedDict\n","import json\n","import time\n","from IPython.display import clear_output"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4040,"status":"ok","timestamp":1667720965264,"user":{"displayName":"Manish Sahoo","userId":"13850174497932657096"},"user_tz":-330},"id":"nlnlWKE7kHJj","outputId":"dc053a61-8af5-4c34-fd13-e9a3d3edf6e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Tqcl6IkLkRtD"},"outputs":[],"source":["if torch.cuda.is_available():\n","  device = torch.device('cuda')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1667720965267,"user":{"displayName":"Manish Sahoo","userId":"13850174497932657096"},"user_tz":-330},"id":"KtXAn2s2kRut","outputId":"774179bb-c7a3-4f34-853e-87c0ae01781e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["device(type='cuda')"]},"metadata":{},"execution_count":5}],"source":["device"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1667720965268,"user":{"displayName":"Manish Sahoo","userId":"13850174497932657096"},"user_tz":-330},"id":"dOvf_j5DkRxS","outputId":"9d6295a2-a31d-49c4-980a-679f2e75569d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Tesla T4'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":6}],"source":["torch.cuda.get_device_name(0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M-yNR4wPdE5y"},"outputs":[],"source":["# dimple -> class 0\n","# background -> class 1\n","# brittle -> class 2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CSK1yZZ2daHE"},"outputs":[],"source":["# for label encoding !!DO NOT RUN AGAIN.\n","\n","# labels_arr = []\n","# for image in os.listdir('/content/drive/MyDrive/BTP/Results- Part-1/dataset/masks'):\n","#   label_arr = cv2.imread(os.path.join('/content/drive/MyDrive/BTP/Results- Part-1/dataset/masks', image), 0)\n","#   labels_arr.append(label_arr)\n","# labels_arr = np.array(labels_arr)\n","# labelencoder = LabelEncoder()\n","# N, H, W = labels_arr.shape\n","# label_reshaped = labels_arr.reshape(-1, )\n","# label_reshaped_encoded = labelencoder.fit_transform(label_reshaped)\n","# labels_arr = label_reshaped_encoded.reshape((N, H, W))\n","\n","# mask_num = 1\n","# for arr in labels_arr:\n","#   mask_name = f\"mask{mask_num}.png\"\n","#   cv2.imwrite(os.path.join('/content/drive/MyDrive/BTP/Results- Part-1/dataset/masks', mask_name), arr)\n","#   mask_num+=1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13426,"status":"ok","timestamp":1667720978677,"user":{"displayName":"Manish Sahoo","userId":"13850174497932657096"},"user_tz":-330},"id":"GMrJr__pkRzQ","outputId":"a239dead-84f6-4817-8dee-e249623dceb1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: gputil in /usr/local/lib/python3.7/dist-packages (1.4.0)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (5.4.8)\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: humanize in /usr/local/lib/python3.7/dist-packages (0.5.1)\n","Gen RAM Free: 12.2 GB  |     Proc size: 447.4 MB\n","GPU RAM Free: 15106MB | Used: 3MB | Util   0% | Total     15109MB\n"]}],"source":["# MEMORY FOOTPRINT SUPPORT LIBRARIES/CODE.\n","!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n","!pip install gputil\n","!pip install psutil\n","!pip install humanize\n","\n","import psutil\n","import humanize\n","import os\n","import GPUtil as GPU\n","\n","GPUs = GPU.getGPUs()\n","gpu = GPUs[0]\n","def printm():\n","    process = psutil.Process(os.getpid())\n","    print(\"Gen RAM Free: \" + humanize.naturalsize(psutil.virtual_memory().available), \" |     Proc size: \" + humanize.naturalsize(process.memory_info().rss))\n","    print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total     {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n","printm()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kaidrSh5kR2s"},"outputs":[],"source":["class ImageData(Dataset):\n","    def __init__(self, images_dir, labels_dir, transform=None):\n","        self.images_dir = images_dir\n","        self.labels_dir = labels_dir\n","        self.images = os.listdir(images_dir)\n","        self.labels = os.listdir(labels_dir)\n","        self.transform = transform\n","        \n","    def __getitem__(self, index):\n","        im = self.images[index]\n","        lab = self.labels[index]\n","        im = np.asarray(io.imread(os.path.join(self.images_dir, im), as_gray=True), dtype=np.float32)\n","        lab = np.asarray(io.imread(os.path.join(self.labels_dir, lab), as_gray=True), dtype=np.float32)\n","        if self.transform:\n","            im = self.transform(im)\n","            lab = self.transform(lab)\n","        return (im, lab)\n","    \n","    def __len__(self):\n","        return len(self.images)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8P5ORZVTkSJh"},"outputs":[],"source":["im_dir = '/content/drive/MyDrive/BTP/Results- Part-1/dataset/images'\n","la_dir = '/content/drive/MyDrive/BTP/Results- Part-1/dataset/masks'\n","\n","#val_la_dir = '/content/drive/MyDrive/spot_finding/val_dir/val_labels'   #change\n","#val_im_dir = '/content/drive/MyDrive/spot_finding/val_dir/val_images'   #change\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"APWlim1LkSLS"},"outputs":[],"source":["class RandomCrop(object):\n","    \"\"\"Crop randomly the image in a sample.\n","\n","    Args:\n","        output_size (tuple or int): Desired output size. If int, square crop\n","            is made.\n","    \"\"\"\n","\n","    def __init__(self, output_size):\n","        assert isinstance(output_size, (int, tuple))\n","        if isinstance(output_size, int):\n","            self.output_size = (output_size, output_size)\n","        else:\n","            assert len(output_size) == 2\n","            self.output_size = output_size\n","\n","    def __call__(self, batch):\n","        image, label = batch[0], batch[1]\n","        h, w = image.shape[2:]\n","        new_h, new_w = self.output_size\n","\n","        if h-new_h>0 and w-new_w>0:\n","          top = np.random.randint(0, h - new_h)\n","          left = np.random.randint(0, w - new_w)\n","\n","          image = image[..., top: top + new_h,\n","                           left: left + new_w]\n","        \n","          label = label[..., top: top + new_h,\n","                           left: left + new_w]\n","        return image, label"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lcXykxeWkrDQ"},"outputs":[],"source":["transform = transforms.Compose([\n","                                transforms.ToTensor()\n","                               ])\n","\n","data = ImageData(images_dir=im_dir, labels_dir=la_dir, transform=transform)\n","\n","#val_data = ImageData(images_dir=val_im_dir, labels_dir=val_la_dir, transform=transform) #change"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":778,"status":"ok","timestamp":1667720979427,"user":{"displayName":"Manish Sahoo","userId":"13850174497932657096"},"user_tz":-330},"id":"_VOLtAgQkrFv","outputId":"f31ba3e5-2fcf-4076-ff97-c52feec38e7b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch.autograd.grad_mode.set_grad_enabled at 0x7f483c22f3d0>"]},"metadata":{},"execution_count":14}],"source":["def double_conv(ic, oc):\n","    conv = nn.Sequential(\n","                         nn.Conv2d(in_channels=ic, out_channels=oc, kernel_size=3, padding='same'),\n","                         nn.ReLU(inplace=True),\n","                         nn.Conv2d(in_channels=oc, out_channels=oc, kernel_size=3, padding='same'),\n","                         nn.ReLU(inplace=True)\n","                        )\n","    \n","    return conv\n","\n","def four_conv(ic, oc):\n","    conv = nn.Sequential(\n","                         nn.Conv2d(in_channels=ic, out_channels=oc, kernel_size=3, padding='same'),\n","                         nn.ReLU(inplace=True),\n","                         nn.Conv2d(in_channels=oc, out_channels=oc, kernel_size=3, padding='same'),\n","                         nn.ReLU(inplace=True),\n","                         nn.Conv2d(in_channels=oc, out_channels=oc, kernel_size=3, padding='same'),\n","                         nn.ReLU(inplace=True)\n","    )\n","    return conv\n","\n","###############------MODIFIED U-NET WITH ATTENTION GATES------##################\n","class UNet(nn.Module):\n","    def __init__(self):\n","        super(UNet, self).__init__()\n","        self.dconv1 = double_conv(1, 64)\n","        self.dconv2 = double_conv(64, 128)\n","        self.fourconv3 = four_conv(128, 256)\n","        self.fourconv4 = four_conv(256, 512)\n","        self.fourconv5 = four_conv(512, 1024)\n","        self.fourconv6 = four_conv(1024, 512) \n","        self.fourconv7 = four_conv(512, 256)\n","        self.fourconv8 = four_conv(256, 128)\n","        self.dconv9 = double_conv(128, 64)\n","        self.dconv10 = double_conv(960, 64)\n","        #OUTPUT CHANNELS CHANGED TO 3.\n","        self.conv1 = nn.Sequential(\n","                                   nn.Conv2d(in_channels=64, out_channels=3, kernel_size=1),\n","                                   nn.Softmax(dim=1) \n","                                  )\n","        self.upconv1 = nn.Sequential(\n","                                     nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=2, stride=2,\n","                                                        padding=0),\n","                                     nn.ReLU(inplace=True)\n","                                    )\n","        self.upconv2 = nn.Sequential(\n","                                     nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=2, stride=2,\n","                                                        padding=0),\n","                                     nn.ReLU(inplace=True)\n","                                    )\n","        self.upconv3 = nn.Sequential(\n","                                     nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=2, stride=2,\n","                                                        padding=0),\n","                                     nn.ReLU(inplace=True)\n","                                    )\n","        self.upconv4 = nn.Sequential(\n","                                     nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=2, stride=2,\n","                                                        padding=0),\n","                                     nn.ReLU(inplace=True)\n","                                    )\n","        # 256 changed to 192 in next 3 lines\n","        self.upsamp1 = nn.Upsample(size=(320, 320), mode='bicubic', align_corners=True)                           \n","        self.upsamp2 = nn.Upsample(size=(320, 320), mode='bicubic', align_corners=True)\n","        self.upsamp3 = nn.Upsample(size=(320, 320), mode='bicubic', align_corners=True)\n","        \n","        # self.enc_sp_conv1 = nn.Conv2d(in_channels=512, out_channels=1, kernel_size=1)\n","        # self.enc_sp_conv2 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=7,\n","        #                               padding=(3, 3), padding_mode='reflect')\n","        # self.dec_sp_conv1 = nn.Conv2d(in_channels=512, out_channels=1, kernel_size=1)\n","        # self.dec_sp_conv2 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=7,\n","        #                               padding=(3, 3), padding_mode='reflect')\n","        # self.enc_ch_conv1 = nn.Conv2d(in_channels=512, out_channels=64, kernel_size=1) \n","        # self.enc_ch_conv2 = nn.Conv2d(in_channels=512, out_channels=64, kernel_size=1)\n","        # self.dec_ch_conv1 = nn.Conv2d(in_channels=512, out_channels=64, kernel_size=1)\n","        # self.dec_ch_conv2 = nn.Conv2d(in_channels=512, out_channels=64, kernel_size=1)\n","        # self.cmap1 = nn.Sequential(\n","        #                             nn.Conv2d(in_channels=64, out_channels=512, kernel_size=1),\n","        #                             nn.Sigmoid()\n","        #                           )\n","\n","        # self.enc_sp_conv3 = nn.Conv2d(in_channels=256, out_channels=1, kernel_size=1)\n","        # self.enc_sp_conv4 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=7,\n","        #                               padding=(3, 3), padding_mode='reflect')\n","        # self.dec_sp_conv3 = nn.Conv2d(in_channels=256, out_channels=1, kernel_size=1)\n","        # self.dec_sp_conv4 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=7,\n","        #                               padding=(3, 3), padding_mode='reflect')\n","        # self.enc_ch_conv3 = nn.Conv2d(in_channels=256, out_channels=32, kernel_size=1) \n","        # self.enc_ch_conv4 = nn.Conv2d(in_channels=256, out_channels=32, kernel_size=1)\n","        # self.dec_ch_conv3 = nn.Conv2d(in_channels=256, out_channels=32, kernel_size=1)\n","        # self.dec_ch_conv4 = nn.Conv2d(in_channels=256, out_channels=32, kernel_size=1)\n","        # self.cmap2 = nn.Sequential(\n","        #                             nn.Conv2d(in_channels=32, out_channels=256, kernel_size=1),\n","        #                             nn.Sigmoid()\n","        #                           ) \n","\n","        # self.enc_sp_conv5 = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=1)\n","        # self.enc_sp_conv6 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=7,\n","        #                               padding=(3, 3), padding_mode='reflect')\n","        # self.dec_sp_conv5 = nn.Conv2d(in_channels=128, out_channels=1, kernel_size=1)\n","        # self.dec_sp_conv6 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=7,\n","        #                               padding=(3, 3), padding_mode='reflect')\n","        # self.enc_ch_conv5 = nn.Conv2d(in_channels=128, out_channels=16, kernel_size=1) \n","        # self.enc_ch_conv6 = nn.Conv2d(in_channels=128, out_channels=16, kernel_size=1)\n","        # self.dec_ch_conv5 = nn.Conv2d(in_channels=128, out_channels=16, kernel_size=1)\n","        # self.dec_ch_conv6 = nn.Conv2d(in_channels=128, out_channels=16, kernel_size=1)\n","        # self.cmap3 = nn.Sequential(\n","        #                             nn.Conv2d(in_channels=16, out_channels=128, kernel_size=1),\n","        #                             nn.Sigmoid()\n","        #                           ) \n","\n","        # self.enc_sp_conv7 = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1)\n","        # self.enc_sp_conv8 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=7,\n","        #                               padding=(3, 3), padding_mode='reflect')\n","        # self.dec_sp_conv7 = nn.Conv2d(in_channels=64, out_channels=1, kernel_size=1)\n","        # self.dec_sp_conv8 = nn.Conv2d(in_channels=3, out_channels=1, kernel_size=7,\n","        #                               padding=(3, 3), padding_mode='reflect')\n","        # self.enc_ch_conv7 = nn.Conv2d(in_channels=64, out_channels=8, kernel_size=1) \n","        # self.enc_ch_conv8 = nn.Conv2d(in_channels=64, out_channels=8, kernel_size=1)\n","        # self.dec_ch_conv7 = nn.Conv2d(in_channels=64, out_channels=8, kernel_size=1)\n","        # self.dec_ch_conv8 = nn.Conv2d(in_channels=64, out_channels=8, kernel_size=1)\n","        # self.cmap4 = nn.Sequential(\n","        #                             nn.Conv2d(in_channels=8, out_channels=64, kernel_size=1),\n","        #                             nn.Sigmoid()\n","        #                           ) \n","         \n","\n","#####################------FEED FORWARD NETWORK------###########################\n","        \n","    def forward(self, t):\n","        #INPUT LAYER :-\n","        t = t\n","        \n","########################------CONTRACTIVE PATH------############################\n","        \n","        #HIDDEN LAYER(1) :-\n","        t = self.dconv1(t)\n","        t = F.pad(t, pad=(2, 2, 2, 2), mode='reflect')\n","        t_1 = t\n","        t = F.max_pool2d(t, kernel_size=2, stride=2)\n","        ########################################################################\n","        \n","        #HIDDEN LAYER(2) :-\n","        t = self.dconv2(t)\n","        t = F.pad(t, pad=(2, 2, 2, 2), mode='reflect')\n","        t_2 = t\n","        t = F.max_pool2d(t, kernel_size=2, stride=2)\n","        ########################################################################\n","    \n","        #HIDDEN LAYER(3) :-\n","        t = self.fourconv3(t)\n","        t = F.pad(t, pad=(4, 4, 4, 4), mode='reflect')\n","        t_3 = t\n","        t = F.max_pool2d(t, kernel_size=2, stride=2)\n","        ########################################################################\n","        \n","        #HIDDEN LAYER(4) :-\n","        t = self.fourconv4(t)\n","        t = F.pad(t, pad=(4, 4, 4, 4), mode='reflect')\n","        t_4 = t\n","        t = F.max_pool2d(t, kernel_size=2, stride=2)\n","        ########################################################################\n","        \n","        #HIDDEN LAYER(5) :-\n","        t = self.fourconv5(t)\n","        t = F.pad(t, pad=(4, 4, 4, 4), mode='reflect')\n","        ########################################################################\n","\n","\n","\n","##############------EXPANSIVE PATH WITH ATTENTION GATES------###################\n","        # #HIDDEN LAYER(6) :-\n","        t = self.upconv1(t)\n","        # # SPATIAL ATTENTION MAP 1 -->\n","        # enc_mean = F.avg_pool3d(t_4, kernel_size=(t_4.shape[1], 1, 1))\n","        # enc_max = F.max_pool3d(t_4, kernel_size=(t_4.shape[1], 1, 1))\n","        # enc_conv = self.enc_sp_conv1(t_4)\n","        # smap_enc = torch.cat((enc_mean, enc_max, enc_conv), dim=1)\n","        # smap_enc = self.enc_sp_conv2(smap_enc)\n","\n","        # dec_mean = F.avg_pool3d(t, kernel_size=(t.shape[1], 1, 1))\n","        # dec_max = F.max_pool3d(t, kernel_size=(t.shape[1], 1, 1))\n","        # dec_conv = self.dec_sp_conv1(t)\n","        # smap_dec = torch.cat((dec_mean, dec_max, dec_conv), dim=1)\n","        # smap_dec = self.dec_sp_conv2(smap_dec)\n","        \n","        # sp_map1 = torch.sigmoid(smap_dec + smap_enc)\n","        \n","        # #..........................---xxxx---...................................\n","\n","        # # CHANNEL ATTENTION MAP 1 -->\n","        # enc_mean = torch.mean(t_4, (2, 3), keepdim=True)\n","        # enc_max = F.max_pool2d(t_4, kernel_size=(t_4.shape[2], t_4.shape[3]))\n","        # enc_mean = self.enc_ch_conv1(enc_mean)\n","        # enc_max = self.enc_ch_conv2(enc_max)\n","        # cmap_enc = enc_max + enc_mean\n","\n","        # dec_mean = torch.mean(t, (2, 3), keepdim=True)\n","        # dec_max = F.max_pool2d(t, kernel_size=(t.shape[2], t.shape[3]))\n","        # dec_mean = self.dec_ch_conv1(dec_mean)\n","        # dec_max = self.dec_ch_conv2(dec_max)\n","        # cmap_dec = dec_max + dec_mean\n","\n","        # ch_map1 = cmap_enc + cmap_dec\n","        # ch_map1 = self.cmap1(ch_map1)\n","\n","        #..........................---xxxx---...................................\n","\n","\n","        #t_4 = t_4 * sp_map1 * ch_map1\n","        t = torch.cat((t_4, t), dim=1)\n","        t = self.fourconv6(t)\n","        t = F.pad(t, pad=(4, 4, 4, 4), mode='reflect')\n","        t_5 = t\n","        ########################################################################\n","\n","        #HIDDEN LAYER(7) :-\n","        t = self.upconv2(t)\n","        # # SPATIAL ATTENTION MAP 2 -->\n","        # enc_mean = F.avg_pool3d(t_3, kernel_size=(t_3.shape[1], 1, 1))\n","        # enc_max = F.max_pool3d(t_3, kernel_size=(t_3.shape[1], 1, 1))\n","        # enc_conv = self.enc_sp_conv3(t_3)\n","        # smap_enc = torch.cat((enc_mean, enc_max, enc_conv), dim=1)\n","        # smap_enc = self.enc_sp_conv4(smap_enc)\n","\n","        # dec_mean = F.avg_pool3d(t, kernel_size=(t.shape[1], 1, 1))\n","        # dec_max = F.max_pool3d(t, kernel_size=(t.shape[1], 1, 1))\n","        # dec_conv = self.dec_sp_conv3(t)\n","        # smap_dec = torch.cat((dec_mean, dec_max, dec_conv), dim=1)\n","        # smap_dec = self.dec_sp_conv4(smap_dec)\n","        \n","        # sp_map2 = torch.sigmoid(smap_dec + smap_enc)\n","        \n","        # #..........................---xxxx---...................................\n","\n","        # # CHANNEL ATTENTION MAP 2 -->\n","        # enc_mean = torch.mean(t_3, (2, 3), keepdim=True)\n","        # enc_max = F.max_pool2d(t_3, kernel_size=(t_3.shape[2], t_3.shape[3]))\n","        # enc_mean = self.enc_ch_conv3(enc_mean)\n","        # enc_max = self.enc_ch_conv4(enc_max)\n","        # cmap_enc = enc_max + enc_mean\n","\n","        # dec_mean = torch.mean(t, (2, 3), keepdim=True)\n","        # dec_max = F.max_pool2d(t, kernel_size=(t.shape[2], t.shape[3]))\n","        # dec_mean = self.dec_ch_conv3(dec_mean)\n","        # dec_max = self.dec_ch_conv4(dec_max)\n","        # cmap_dec = dec_max + dec_mean\n","\n","        # ch_map2 = cmap_enc + cmap_dec\n","        # ch_map2 = self.cmap2(ch_map2)\n","\n","        # #..........................---xxxx---...................................\n","\n","        #t_3 = t_3 * sp_map2 * ch_map2 \n","        t = torch.cat((t_3, t), dim=1)\n","        t = self.fourconv7(t)\n","        t = F.pad(t, pad=(4, 4, 4, 4), mode='reflect')\n","        t_6 = t\n","        ########################################################################\n","\n","        #HIDDEN LAYER(8) :-\n","        t = self.upconv3(t)\n","        # # SPATIAL ATTENTION MAP 3 -->\n","        # enc_mean = F.avg_pool3d(t_2, kernel_size=(t_2.shape[1], 1, 1))\n","        # enc_max = F.max_pool3d(t_2, kernel_size=(t_2.shape[1], 1, 1))\n","        # enc_conv = self.enc_sp_conv5(t_2)\n","        # smap_enc = torch.cat((enc_mean, enc_max, enc_conv), dim=1)\n","        # smap_enc = self.enc_sp_conv6(smap_enc)\n","\n","        # dec_mean = F.avg_pool3d(t, kernel_size=(t.shape[1], 1, 1))\n","        # dec_max = F.max_pool3d(t, kernel_size=(t.shape[1], 1, 1))\n","        # dec_conv = self.dec_sp_conv5(t)\n","        # smap_dec = torch.cat((dec_mean, dec_max, dec_conv), dim=1)\n","        # smap_dec = self.dec_sp_conv6(smap_dec)\n","        \n","        # sp_map3 = torch.sigmoid(smap_dec + smap_enc)\n","        \n","        # #..........................---xxxx---...................................\n","\n","        # # CHANNEL ATTENTION MAP 3 -->\n","        # enc_mean = torch.mean(t_2, (2, 3), keepdim=True)\n","        # enc_max = F.max_pool2d(t_2, kernel_size=(t_2.shape[2], t_2.shape[3]))\n","        # enc_mean = self.enc_ch_conv5(enc_mean)\n","        # enc_max = self.enc_ch_conv6(enc_max)\n","        # cmap_enc = enc_max + enc_mean\n","\n","        # dec_mean = torch.mean(t, (2, 3), keepdim=True)\n","        # dec_max = F.max_pool2d(t, kernel_size=(t.shape[2], t.shape[3]))\n","        # dec_mean = self.dec_ch_conv5(dec_mean)\n","        # dec_max = self.dec_ch_conv6(dec_max)\n","        # cmap_dec = dec_max + dec_mean\n","\n","        # ch_map3 = cmap_enc + cmap_dec\n","        # ch_map3 = self.cmap3(ch_map3)\n","\n","        # #..........................---xxxx---...................................\n","\n","        #t_2 = t_2 * sp_map3 * ch_map3 \n","        t = torch.cat((t_2, t), dim=1)\n","        t = self.fourconv8(t)\n","        t = F.pad(t, pad=(4, 4, 4, 4), mode='reflect')\n","        t_7 = t\n","        ########################################################################\n","\n","        #HIDDEN LAYER(9) :-\n","        t = self.upconv4(t)\n","        # # SPATIAL ATTENTION MAP 4 -->\n","        # enc_mean = F.avg_pool3d(t_1, kernel_size=(t_1.shape[1], 1, 1))\n","        # enc_max = F.max_pool3d(t_1, kernel_size=(t_1.shape[1], 1, 1))\n","        # enc_conv = self.enc_sp_conv7(t_1)\n","        # smap_enc = torch.cat((enc_mean, enc_max, enc_conv), dim=1)\n","        # smap_enc = self.enc_sp_conv8(smap_enc)\n","\n","        # dec_mean = F.avg_pool3d(t, kernel_size=(t.shape[1], 1, 1))\n","        # dec_max = F.max_pool3d(t, kernel_size=(t.shape[1], 1, 1))\n","        # dec_conv = self.dec_sp_conv7(t)\n","        # smap_dec = torch.cat((dec_mean, dec_max, dec_conv), dim=1)\n","        # smap_dec = self.dec_sp_conv8(smap_dec)\n","        \n","        # sp_map4 = torch.sigmoid(smap_dec + smap_enc)\n","        \n","        # #..........................---xxxx---...................................\n","\n","        # # CHANNEL ATTENTION MAP 4 -->\n","        # enc_mean = torch.mean(t_1, (2, 3), keepdim=True)\n","        # enc_max = F.max_pool2d(t_1, kernel_size=(t_1.shape[2], t_1.shape[3]))\n","        # enc_mean = self.enc_ch_conv7(enc_mean)\n","        # enc_max = self.enc_ch_conv8(enc_max)\n","        # cmap_enc = enc_max + enc_mean\n","\n","        # dec_mean = torch.mean(t, (2, 3), keepdim=True)\n","        # dec_max = F.max_pool2d(t, kernel_size=(t.shape[2], t.shape[3]))\n","        # dec_mean = self.dec_ch_conv7(dec_mean)\n","        # dec_max = self.dec_ch_conv8(dec_max)\n","        # cmap_dec = dec_max + dec_mean\n","\n","        # ch_map4 = cmap_enc + cmap_dec\n","        # ch_map4 = self.cmap4(ch_map4)\n","\n","        # #..........................---xxxx---...................................\n","\n","        #t_1 = t_1 * sp_map4 * ch_map4 \n","        t = torch.cat((t_1, t), dim=1)\n","        t = self.dconv9(t)\n","        t = F.pad(t, pad=(2, 2, 2, 2), mode='reflect')\n","        t_8 = t\n","        ########################################################################\n","\n","        #HIDDEN LAYER(10) OR FUSION LAYER :-\n","        t_5 = self.upsamp1(t_5)\n","        t_6 = self.upsamp2(t_6)\n","        t_7 = self.upsamp3(t_7)\n","        t = torch.cat((t_8, t_7, t_6, t_5), dim=1)\n","        t = self.dconv10(t)\n","        t = F.pad(t, pad=(2, 2, 2, 2), mode='reflect')\n","        ########################################################################\n","\n","        #OUTPUT LAYER\n","        t = self.conv1(t)\n","        ########################################################################\n","        \n","        return t\n","\n","torch.set_grad_enabled(True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6bCARa5KkrJ0"},"outputs":[],"source":["# RUN MANAGER CLASS TO KEEP TRACK OF VARIOUS PARAMETERS WHILE TRAINING.\n","class RunManager():\n","    def __init__(self):\n","        self.epoch_count = 0\n","        self.epoch_loss = 0\n","        self.epoch_val_loss = 0 #change\n","        self.accuracy = 0\n","        self.epoch_start_time = None\n","        \n","        self.run_params = None\n","        self.run_count = 0\n","        self.run_data = []\n","        self.run_start_time = None\n","        \n","        self.network = None\n","        self.train_loader = None #change\n","        #self.val_loader = None    #change\n","        \n","    def begin_run(self, run, network, train_loader):\n","        self.run_start_time = time.time()\n","        \n","        self.run_params = run\n","        self.run_count += 1\n","        \n","        self.network = network\n","        self.train_loader = train_loader\n","        #self.val_loader = val_loader\n","        #self.tb = SummaryWriter(comment=f'-{run}')\n","        \n","        images, labels = next(iter(self.train_loader)) #change\n","        grid = torchvision.utils.make_grid(images)\n","\n","        #self.tb.add_image('images', grid)\n","        #self.tb.add_graph(self.network, images.to(getattr(run, 'device', 'cpu')))\n","         #check if device is present in run else default to 'cpu'. \n","        \n","\n","    def end_run(self):\n","        self.epoch_count=0\n","        #self.tb.close()    \n","        \n","    def begin_epoch(self):\n","        self.epoch_start_time = time.time()\n","        self.epoch_count+=1\n","        self.epoch_loss = 0\n","        #self.epoch_val_loss = 0\n","        self.f1_score = 0\n","        #self.f1_score2 = 0\n","        #self.f1_score3 = 0\n","        #self.fpr1 = 0\n","        #self.fpr2 = 0\n","        #self.fpr3 = 0\n","        self.IOU = 0\n","        #self.IOU2 = 0\n","        #self.IOU3 = 0\n","        \n","        #self.tpr = 0\n","        \n","    def end_epoch(self):\n","        epoch_duration = time.time() - self.epoch_start_time\n","        run_duration = time.time() - self.run_start_time\n","        \n","        loss = self.epoch_loss/len(self.train_loader.dataset)\n","        #val_loss = self.epoch_val_loss/len(self.val_loader.dataset)\n","        f1_score = self.f1_score/len(self.train_loader.dataset)\n","        #f1_score2 = self.f1_score2/len(self.train_loader.dataset)\n","        #f1_score3 = self.f1_score3/len(self.train_loader.dataset)\n","        #fpr1 = self.fpr1/len(self.train_loader.dataset)\n","        #fpr2 = self.fpr2/len(self.train_loader.dataset)\n","        #fpr3 = self.fpr3/len(self.train_loader.dataset)\n","        IOU = self.IOU/len(self.train_loader.dataset)\n","        #IOU2 = self.IOU2/len(self.train_loader.dataset)\n","        #IOU3 = self.IOU3/len(self.train_loader.dataset)\n","        #self.tb.add_scalar('loss', self.epoch_loss, self.epoch_count)\n","        #self.tb.add_scalar('dice score', self.epoch_loss, self.epoch_count)\n","        \n","        #for name, weight in self.network.named_parameters():\n","            #self.tb.add_histogram(name, weight, self.epoch_count)\n","            #self.tb.add_histogram(f'{name}.grad', weight.grad, self.epoch_count)\n","            \n","        results = OrderedDict()\n","        results['run'] = self.run_count\n","        results['epoch'] = self.epoch_count\n","        results['loss'] = loss\n","        #results['val loss'] = val_loss\n","        results['F1 score'] = f1_score/12\n","        #results['F1 score-2'] = f1_score2\n","        #results['F1 score-3'] = f1_score3\n","        #results['FPR-1'] = fpr1\n","        #results['FPR-2'] = fpr2\n","        #results['FPR-3'] = fpr3\n","        results['IOU'] = IOU/12\n","        #results['IOU-2'] = IOU2\n","        #results['IOU-3'] = IOU3\n","        results['epoch_duration'] = epoch_duration\n","        results['run_duration'] = run_duration\n","        \n","        for k,v in self.run_params._asdict().items():\n","            results[k] = v\n","        \n","        self.run_data.append(results)\n","        df = pd.DataFrame.from_dict(self.run_data, orient='columns')\n","        \n","        clear_output(wait=True)\n","        display(df)\n","        \n","    def track_loss(self, loss):\n","        self.epoch_loss += loss*self.train_loader.batch_size\n","    \n","    #def track_val_loss(self, val_loss):\n","        #self.epoch_val_loss += val_loss.item()*self.val_loader.batch_size\n","        \n","    def track_accuracy(self, preds, labels): #tracks various metrics being evaluated\n","        self.f1_score += self._get_accuracy(preds, labels)[0]\n","        #self.f1_score2 += self._get_accuracy(preds, labels)[0][1]\n","        #self.f1_score3 += self._get_accuracy(preds, labels)[0][2]\n","        #self.fpr1 += self._get_accuracy(preds, labels)[1][0]\n","        #self.fpr2 += self._get_accuracy(preds, labels)[1][1]\n","        #self.fpr3 += self._get_accuracy(preds, labels)[1][2]\n","        self.IOU += self._get_accuracy(preds, labels)[1]\n","        #self.IOU2 += self._get_accuracy(preds, labels)[1][1]\n","        #self.IOU3 += self._get_accuracy(preds, labels)[1][2]\n","\n","        \n","    @torch.no_grad()\n","    def _get_accuracy(self, preds, labels):\n","        p = preds.to('cpu')\n","        l = labels.to('cpu')\n","        #p = (p - p.min())/(p.max() - p.min())\n","        p = p.type(torch.FloatTensor)\n","        l = l.type(torch.LongTensor)\n","        #output = stat_scores(p, l, threshold=0.5, reduce='macro', mdmc_reduce='global', num_classes=3)\n","        #print(output)\n","        #fpr = [output[i, 1].item()/(output[i, 1].item()+output[i, 2].item()) for i in range(3)]\n","        #p = p > 0.5\n","        #p = p.to(torch.int64)\n","        score = multiclass_f1_score(p, l, num_classes=3, average='macro', multidim_average='global')\n","        IOU = multiclass_jaccard_index(p, l, num_classes=3, average='macro')\n","        return (score.item(), IOU.item())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cKpVoeL9krMm"},"outputs":[],"source":["# RUN BUILDER CLASS TO GENERATE RUNS WITH DIFFERENT PARAMETERS.\n","class RunBuilder():\n","    @staticmethod\n","    def get_runs(params):\n","        Run = namedtuple('Run', params.keys())\n","        \n","        runs = []\n","        for v in product(*params.values()):\n","            runs.append(Run(*v))\n","            \n","        return runs\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"u1w3C4UokSOX","executionInfo":{"status":"ok","timestamp":1667724840548,"user_tz":-330,"elapsed":3861124,"user":{"displayName":"Manish Sahoo","userId":"13850174497932657096"}},"outputId":"7693fc2c-2a16-4686-90b6-ea115004bff7"},"outputs":[{"output_type":"display_data","data":{"text/plain":["    run  epoch       loss  F1 score       IOU  epoch_duration  run_duration  \\\n","0     1      1  13.034403  0.190072  0.162056       77.855876     78.094129   \n","1     1      2  12.641655  0.229018  0.198737       73.617357    151.735190   \n","2     1      3  12.326859  0.229018  0.198737       74.805317    226.555817   \n","3     1      4  12.089453  0.229018  0.198737       75.619549    302.190234   \n","4     1      5  11.915960  0.229018  0.198737       77.116025    379.324457   \n","5     1      6  11.795634  0.229018  0.198737       76.286240    455.628296   \n","6     1      7  11.718742  0.229018  0.198737       76.127679    531.771644   \n","7     1      8  11.673036  0.229018  0.198737       76.226599    608.014246   \n","8     1      9  11.646774  0.229018  0.198737       76.712602    684.746346   \n","9     1     10  11.631729  0.229018  0.198737       76.495413    761.259951   \n","10    1     11  11.623061  0.229018  0.198737       76.480582    837.757865   \n","11    1     12  11.617977  0.229018  0.198737       76.424896    914.202387   \n","12    1     13  11.614876  0.229018  0.198737       76.578537    990.798956   \n","13    1     14  11.612935  0.229018  0.198737       76.563485   1067.381308   \n","14    1     15  11.611652  0.229018  0.198737       76.437820   1143.838743   \n","15    1     16  11.610747  0.229018  0.198737       76.499071   1220.356506   \n","16    1     17  11.610056  0.229018  0.198737       76.425915   1296.801323   \n","17    1     18  11.609482  0.229018  0.198737       76.629342   1373.452314   \n","18    1     19  11.608979  0.229018  0.198737       77.674565   1451.151229   \n","19    1     20  11.608519  0.229018  0.198737       77.679751   1528.850710   \n","20    1     21  11.608079  0.229018  0.198737       77.711166   1606.585037   \n","21    1     22  11.607658  0.229018  0.198737       77.559530   1684.167031   \n","22    1     23  11.607245  0.229018  0.198737       77.715279   1761.902388   \n","23    1     24  11.606837  0.229018  0.198737       77.670073   1839.592934   \n","24    1     25  11.606435  0.229018  0.198737       77.775928   1917.394395   \n","25    1     26  11.606039  0.229018  0.198737       77.565082   1994.983019   \n","26    1     27  11.605647  0.229018  0.198737       77.583274   2072.590072   \n","27    1     28  11.605262  0.229018  0.198737       77.661292   2150.275614   \n","28    1     29  11.604877  0.229018  0.198737       77.687528   2227.984444   \n","29    1     30  11.604503  0.229018  0.198737       77.705852   2305.715288   \n","30    1     31  11.604136  0.229018  0.198737       77.561396   2383.302172   \n","31    1     32  11.603777  0.229018  0.198737       77.363484   2460.689392   \n","32    1     33  11.603427  0.229018  0.198737       77.590164   2538.302698   \n","33    1     34  11.603084  0.229018  0.198737       77.669810   2616.000052   \n","34    1     35  11.602750  0.229018  0.198737       77.523137   2693.548429   \n","35    1     36  11.602422  0.229018  0.198737       77.596317   2771.171127   \n","36    1     37  11.602099  0.229018  0.198737       77.656439   2848.852974   \n","37    1     38  11.601781  0.229018  0.198737       77.577280   2926.458292   \n","38    1     39  11.601470  0.229018  0.198737       77.619130   3004.102185   \n","39    1     40  11.601166  0.229018  0.198737       77.712701   3081.839640   \n","40    1     41  11.600871  0.229018  0.198737       77.743968   3159.612545   \n","41    1     42  11.600575  0.229018  0.198737       77.613933   3237.252520   \n","42    1     43  11.600280  0.229018  0.198737       77.636463   3314.916237   \n","43    1     44  11.599988  0.229018  0.198737       77.646709   3392.589478   \n","44    1     45  11.599705  0.229018  0.198737       77.613772   3470.229607   \n","45    1     46  11.599433  0.229018  0.198737       77.558735   3547.819236   \n","46    1     47  11.599183  0.229018  0.198737       77.550065   3625.396257   \n","47    1     48  11.598931  0.229018  0.198737       77.640038   3703.062915   \n","48    1     49  11.598661  0.229018  0.198737       77.643373   3780.735890   \n","49    1     50  11.598369  0.229018  0.198737       77.562140   3858.326551   \n","\n","    batch_size    lr  num_workers  momentum device  \n","0            1  0.01            2       0.5   cuda  \n","1            1  0.01            2       0.5   cuda  \n","2            1  0.01            2       0.5   cuda  \n","3            1  0.01            2       0.5   cuda  \n","4            1  0.01            2       0.5   cuda  \n","5            1  0.01            2       0.5   cuda  \n","6            1  0.01            2       0.5   cuda  \n","7            1  0.01            2       0.5   cuda  \n","8            1  0.01            2       0.5   cuda  \n","9            1  0.01            2       0.5   cuda  \n","10           1  0.01            2       0.5   cuda  \n","11           1  0.01            2       0.5   cuda  \n","12           1  0.01            2       0.5   cuda  \n","13           1  0.01            2       0.5   cuda  \n","14           1  0.01            2       0.5   cuda  \n","15           1  0.01            2       0.5   cuda  \n","16           1  0.01            2       0.5   cuda  \n","17           1  0.01            2       0.5   cuda  \n","18           1  0.01            2       0.5   cuda  \n","19           1  0.01            2       0.5   cuda  \n","20           1  0.01            2       0.5   cuda  \n","21           1  0.01            2       0.5   cuda  \n","22           1  0.01            2       0.5   cuda  \n","23           1  0.01            2       0.5   cuda  \n","24           1  0.01            2       0.5   cuda  \n","25           1  0.01            2       0.5   cuda  \n","26           1  0.01            2       0.5   cuda  \n","27           1  0.01            2       0.5   cuda  \n","28           1  0.01            2       0.5   cuda  \n","29           1  0.01            2       0.5   cuda  \n","30           1  0.01            2       0.5   cuda  \n","31           1  0.01            2       0.5   cuda  \n","32           1  0.01            2       0.5   cuda  \n","33           1  0.01            2       0.5   cuda  \n","34           1  0.01            2       0.5   cuda  \n","35           1  0.01            2       0.5   cuda  \n","36           1  0.01            2       0.5   cuda  \n","37           1  0.01            2       0.5   cuda  \n","38           1  0.01            2       0.5   cuda  \n","39           1  0.01            2       0.5   cuda  \n","40           1  0.01            2       0.5   cuda  \n","41           1  0.01            2       0.5   cuda  \n","42           1  0.01            2       0.5   cuda  \n","43           1  0.01            2       0.5   cuda  \n","44           1  0.01            2       0.5   cuda  \n","45           1  0.01            2       0.5   cuda  \n","46           1  0.01            2       0.5   cuda  \n","47           1  0.01            2       0.5   cuda  \n","48           1  0.01            2       0.5   cuda  \n","49           1  0.01            2       0.5   cuda  "],"text/html":["\n","  <div id=\"df-26e2b49f-1063-4e8f-af9e-50e41f627923\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>run</th>\n","      <th>epoch</th>\n","      <th>loss</th>\n","      <th>F1 score</th>\n","      <th>IOU</th>\n","      <th>epoch_duration</th>\n","      <th>run_duration</th>\n","      <th>batch_size</th>\n","      <th>lr</th>\n","      <th>num_workers</th>\n","      <th>momentum</th>\n","      <th>device</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>13.034403</td>\n","      <td>0.190072</td>\n","      <td>0.162056</td>\n","      <td>77.855876</td>\n","      <td>78.094129</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>12.641655</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>73.617357</td>\n","      <td>151.735190</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1</td>\n","      <td>3</td>\n","      <td>12.326859</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>74.805317</td>\n","      <td>226.555817</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1</td>\n","      <td>4</td>\n","      <td>12.089453</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>75.619549</td>\n","      <td>302.190234</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1</td>\n","      <td>5</td>\n","      <td>11.915960</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.116025</td>\n","      <td>379.324457</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>6</td>\n","      <td>11.795634</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>76.286240</td>\n","      <td>455.628296</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>1</td>\n","      <td>7</td>\n","      <td>11.718742</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>76.127679</td>\n","      <td>531.771644</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>1</td>\n","      <td>8</td>\n","      <td>11.673036</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>76.226599</td>\n","      <td>608.014246</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>1</td>\n","      <td>9</td>\n","      <td>11.646774</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>76.712602</td>\n","      <td>684.746346</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1</td>\n","      <td>10</td>\n","      <td>11.631729</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>76.495413</td>\n","      <td>761.259951</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>1</td>\n","      <td>11</td>\n","      <td>11.623061</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>76.480582</td>\n","      <td>837.757865</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>1</td>\n","      <td>12</td>\n","      <td>11.617977</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>76.424896</td>\n","      <td>914.202387</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>1</td>\n","      <td>13</td>\n","      <td>11.614876</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>76.578537</td>\n","      <td>990.798956</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>1</td>\n","      <td>14</td>\n","      <td>11.612935</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>76.563485</td>\n","      <td>1067.381308</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>1</td>\n","      <td>15</td>\n","      <td>11.611652</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>76.437820</td>\n","      <td>1143.838743</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>1</td>\n","      <td>16</td>\n","      <td>11.610747</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>76.499071</td>\n","      <td>1220.356506</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>1</td>\n","      <td>17</td>\n","      <td>11.610056</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>76.425915</td>\n","      <td>1296.801323</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>1</td>\n","      <td>18</td>\n","      <td>11.609482</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>76.629342</td>\n","      <td>1373.452314</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>1</td>\n","      <td>19</td>\n","      <td>11.608979</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.674565</td>\n","      <td>1451.151229</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>1</td>\n","      <td>20</td>\n","      <td>11.608519</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.679751</td>\n","      <td>1528.850710</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>1</td>\n","      <td>21</td>\n","      <td>11.608079</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.711166</td>\n","      <td>1606.585037</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>1</td>\n","      <td>22</td>\n","      <td>11.607658</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.559530</td>\n","      <td>1684.167031</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>1</td>\n","      <td>23</td>\n","      <td>11.607245</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.715279</td>\n","      <td>1761.902388</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>23</th>\n","      <td>1</td>\n","      <td>24</td>\n","      <td>11.606837</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.670073</td>\n","      <td>1839.592934</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>24</th>\n","      <td>1</td>\n","      <td>25</td>\n","      <td>11.606435</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.775928</td>\n","      <td>1917.394395</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>25</th>\n","      <td>1</td>\n","      <td>26</td>\n","      <td>11.606039</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.565082</td>\n","      <td>1994.983019</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>1</td>\n","      <td>27</td>\n","      <td>11.605647</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.583274</td>\n","      <td>2072.590072</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>27</th>\n","      <td>1</td>\n","      <td>28</td>\n","      <td>11.605262</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.661292</td>\n","      <td>2150.275614</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>1</td>\n","      <td>29</td>\n","      <td>11.604877</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.687528</td>\n","      <td>2227.984444</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>29</th>\n","      <td>1</td>\n","      <td>30</td>\n","      <td>11.604503</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.705852</td>\n","      <td>2305.715288</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>1</td>\n","      <td>31</td>\n","      <td>11.604136</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.561396</td>\n","      <td>2383.302172</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>1</td>\n","      <td>32</td>\n","      <td>11.603777</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.363484</td>\n","      <td>2460.689392</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>32</th>\n","      <td>1</td>\n","      <td>33</td>\n","      <td>11.603427</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.590164</td>\n","      <td>2538.302698</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>33</th>\n","      <td>1</td>\n","      <td>34</td>\n","      <td>11.603084</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.669810</td>\n","      <td>2616.000052</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>1</td>\n","      <td>35</td>\n","      <td>11.602750</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.523137</td>\n","      <td>2693.548429</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>1</td>\n","      <td>36</td>\n","      <td>11.602422</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.596317</td>\n","      <td>2771.171127</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>1</td>\n","      <td>37</td>\n","      <td>11.602099</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.656439</td>\n","      <td>2848.852974</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>37</th>\n","      <td>1</td>\n","      <td>38</td>\n","      <td>11.601781</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.577280</td>\n","      <td>2926.458292</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>1</td>\n","      <td>39</td>\n","      <td>11.601470</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.619130</td>\n","      <td>3004.102185</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>39</th>\n","      <td>1</td>\n","      <td>40</td>\n","      <td>11.601166</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.712701</td>\n","      <td>3081.839640</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>40</th>\n","      <td>1</td>\n","      <td>41</td>\n","      <td>11.600871</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.743968</td>\n","      <td>3159.612545</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>1</td>\n","      <td>42</td>\n","      <td>11.600575</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.613933</td>\n","      <td>3237.252520</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>1</td>\n","      <td>43</td>\n","      <td>11.600280</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.636463</td>\n","      <td>3314.916237</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>1</td>\n","      <td>44</td>\n","      <td>11.599988</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.646709</td>\n","      <td>3392.589478</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>44</th>\n","      <td>1</td>\n","      <td>45</td>\n","      <td>11.599705</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.613772</td>\n","      <td>3470.229607</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>1</td>\n","      <td>46</td>\n","      <td>11.599433</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.558735</td>\n","      <td>3547.819236</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>46</th>\n","      <td>1</td>\n","      <td>47</td>\n","      <td>11.599183</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.550065</td>\n","      <td>3625.396257</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>47</th>\n","      <td>1</td>\n","      <td>48</td>\n","      <td>11.598931</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.640038</td>\n","      <td>3703.062915</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>48</th>\n","      <td>1</td>\n","      <td>49</td>\n","      <td>11.598661</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.643373</td>\n","      <td>3780.735890</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>1</td>\n","      <td>50</td>\n","      <td>11.598369</td>\n","      <td>0.229018</td>\n","      <td>0.198737</td>\n","      <td>77.562140</td>\n","      <td>3858.326551</td>\n","      <td>1</td>\n","      <td>0.01</td>\n","      <td>2</td>\n","      <td>0.5</td>\n","      <td>cuda</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-26e2b49f-1063-4e8f-af9e-50e41f627923')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-26e2b49f-1063-4e8f-af9e-50e41f627923 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-26e2b49f-1063-4e8f-af9e-50e41f627923');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}}],"source":["# MAIN TRAINING LOOP WITH DIFFERENT FOR DIFFERENT RUNS.\n","\n","\n","params = OrderedDict(\n","                     batch_size = [1],\n","                     lr = [0.01],\n","                     num_workers = [2],\n","                     momentum = [0.5],\n","                     device = ['cuda']\n","                    )\n","\n","m = RunManager()\n","for run in RunBuilder.get_runs(params):\n","    \n","    device = torch.device(run.device)\n","    network = UNet().to(device)     \n","    train_loader = DataLoader(data, batch_size=run.batch_size, shuffle=False, num_workers=run.num_workers)\n","    optimizer = optim.SGD(network.parameters(), lr = run.lr, momentum=run.lr)\n","    crop = RandomCrop((960, 1280))   \n","    m.begin_run(run, network, train_loader)\n","\n","    for epoch in range(50):\n","        \n","        m.begin_epoch()\n","        for batch in train_loader:\n","            images, labels = crop(batch)\n","            image_tiles = torch.from_numpy(util.view_as_blocks(images.numpy(), (1, 1, 320, 320))).to(device)\n","            label_tiles = torch.from_numpy(util.view_as_blocks(labels.numpy(), (1, 1, 320, 320))).to(device)\n","            epoch_loss = 0\n","            \n","            #labels_batch =  (torch.zeros((3, 4, 1, 1, 320, 320)).to(device)).type(torch.cuda.LongTensor)\n","            for (i,j) in product(range(image_tiles.shape[2]), range(image_tiles.shape[3])):\n","                image_tile = image_tiles[0, 0, i, j, ...]\n","                label_tile = label_tiles[0, 0, i, j, ...]\n","                label_tile = torch.squeeze(label_tile, 0).type(torch.cuda.LongTensor)\n","                image_tile = (image_tile-torch.mean(image_tile))/torch.std(image_tile)\n","                pred_tile = network(image_tile)\n","                #images_batch[i, j, ...] = image_tile\n","                #labels_batch[i, j, ...] = label_tile\n","                pred_tile = pred_tile.to(device)\n","                pred_tile = pred_tile.type(torch.cuda.FloatTensor)\n","                loss = F.cross_entropy(pred_tile, label_tile, reduction='mean')\n","                epoch_loss += loss.mean().item()\n","\n","                optimizer.zero_grad()\n","                loss.backward(loss)\n","                optimizer.step()\n","                m.track_accuracy(pred_tile, label_tile)\n","      \n","            m.track_loss(epoch_loss)\n","            \n","\n","\n","\n","            \n","            #images_batch = torch.reshape(images_batch, [-1, 1, 320, 320])\n","            #labels_batch = torch.reshape(labels_batch, [-1, 320, 320])\n","            #images = images.to(device)\n","            #labels = labels.to(device)\n","            #preds = network(images_batch)\n","            #preds = preds.to(device)\n","            #preds = preds.type(torch.cuda.FloatTensor)\n","            #labels = torch.squeeze(labels, 0).type(torch.cuda.LongTensor) \n","            #loss = F.cross_entropy(preds, labels_batch, reduction='mean')\n","            \n","            #optimizer.zero_grad()\n","            #loss.backward(loss)\n","            #optimizer.step()\n","            #m.track_loss(loss.mean())\n","            #m.track_accuracy(preds, labels_batch)\n","\n","        m.end_epoch()  \n","    m.end_run()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gyajjtMHSie_"},"outputs":[],"source":["im_dir = '/content/drive/MyDrive/BTP/Results- Part-1/dataset/images'\n","image = io.imread(os.path.join(im_dir, 'image1.png'))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xg0cgNLplI6q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1667671706656,"user_tz":-330,"elapsed":1156,"user":{"displayName":"Manish Sahoo","userId":"13850174497932657096"}},"outputId":"fefe3cd8-6adc-4d6c-d3a4-e24c21510432"},"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([[[[0.3216, 0.3216, 0.3216,  ..., 0.3216, 0.3217, 0.3217],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3216, 0.3217, 0.3217],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3217, 0.3217, 0.3217],\n","          ...,\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3217, 0.3217, 0.3217],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3217, 0.3217, 0.3217],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3217, 0.3217, 0.3216]],\n","\n","         [[0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3472, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3474,  ..., 0.3473, 0.3473, 0.3473],\n","          ...,\n","          [0.3473, 0.3473, 0.3474,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473]],\n","\n","         [[0.3311, 0.3311, 0.3311,  ..., 0.3311, 0.3310, 0.3310],\n","          [0.3311, 0.3311, 0.3310,  ..., 0.3310, 0.3310, 0.3310],\n","          [0.3310, 0.3311, 0.3309,  ..., 0.3310, 0.3310, 0.3310],\n","          ...,\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3310, 0.3310, 0.3310],\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3310, 0.3310, 0.3310],\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3310, 0.3310, 0.3310]]]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","tensor([[[[0.3216, 0.3216, 0.3216,  ..., 0.3217, 0.3217, 0.3216],\n","          [0.3216, 0.3216, 0.3216,  ..., 0.3216, 0.3216, 0.3216],\n","          [0.3217, 0.3216, 0.3217,  ..., 0.3216, 0.3216, 0.3216],\n","          ...,\n","          [0.3217, 0.3217, 0.3216,  ..., 0.3216, 0.3217, 0.3216],\n","          [0.3216, 0.3216, 0.3216,  ..., 0.3217, 0.3217, 0.3216],\n","          [0.3216, 0.3216, 0.3217,  ..., 0.3217, 0.3217, 0.3216]],\n","\n","         [[0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          ...,\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3474, 0.3473, 0.3474,  ..., 0.3473, 0.3473, 0.3474],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3474]],\n","\n","         [[0.3311, 0.3311, 0.3311,  ..., 0.3310, 0.3311, 0.3311],\n","          [0.3310, 0.3310, 0.3311,  ..., 0.3311, 0.3311, 0.3311],\n","          [0.3310, 0.3310, 0.3311,  ..., 0.3311, 0.3311, 0.3311],\n","          ...,\n","          [0.3310, 0.3310, 0.3311,  ..., 0.3310, 0.3310, 0.3311],\n","          [0.3310, 0.3311, 0.3310,  ..., 0.3310, 0.3310, 0.3310],\n","          [0.3311, 0.3311, 0.3310,  ..., 0.3311, 0.3310, 0.3310]]]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","tensor([[[[0.3217, 0.3217, 0.3217,  ..., 0.3216, 0.3217, 0.3217],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3216, 0.3217, 0.3217],\n","          [0.3217, 0.3217, 0.3216,  ..., 0.3216, 0.3216, 0.3216],\n","          ...,\n","          [0.3217, 0.3218, 0.3217,  ..., 0.3216, 0.3216, 0.3217],\n","          [0.3217, 0.3218, 0.3217,  ..., 0.3216, 0.3217, 0.3217],\n","          [0.3217, 0.3218, 0.3217,  ..., 0.3216, 0.3217, 0.3217]],\n","\n","         [[0.3473, 0.3474, 0.3473,  ..., 0.3473, 0.3473, 0.3472],\n","          [0.3474, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3472, 0.3472, 0.3472],\n","          ...,\n","          [0.3472, 0.3472, 0.3472,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3472, 0.3472, 0.3472,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3472, 0.3472, 0.3473,  ..., 0.3473, 0.3473, 0.3473]],\n","\n","         [[0.3310, 0.3310, 0.3310,  ..., 0.3311, 0.3311, 0.3311],\n","          [0.3309, 0.3310, 0.3310,  ..., 0.3311, 0.3310, 0.3310],\n","          [0.3310, 0.3310, 0.3311,  ..., 0.3311, 0.3311, 0.3311],\n","          ...,\n","          [0.3310, 0.3310, 0.3311,  ..., 0.3311, 0.3310, 0.3310],\n","          [0.3311, 0.3311, 0.3311,  ..., 0.3311, 0.3310, 0.3310],\n","          [0.3311, 0.3311, 0.3310,  ..., 0.3311, 0.3310, 0.3310]]]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","tensor([[[[0.3216, 0.3216, 0.3216,  ..., 0.3217, 0.3217, 0.3217],\n","          [0.3216, 0.3216, 0.3215,  ..., 0.3217, 0.3217, 0.3217],\n","          [0.3216, 0.3216, 0.3215,  ..., 0.3217, 0.3217, 0.3217],\n","          ...,\n","          [0.3216, 0.3217, 0.3216,  ..., 0.3217, 0.3217, 0.3217],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3217, 0.3217, 0.3216],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3217, 0.3217, 0.3216]],\n","\n","         [[0.3474, 0.3474, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3474, 0.3474, 0.3473,  ..., 0.3473, 0.3473, 0.3472],\n","          [0.3474, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3472],\n","          ...,\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473]],\n","\n","         [[0.3311, 0.3311, 0.3311,  ..., 0.3310, 0.3310, 0.3311],\n","          [0.3310, 0.3310, 0.3311,  ..., 0.3310, 0.3310, 0.3310],\n","          [0.3310, 0.3310, 0.3311,  ..., 0.3310, 0.3310, 0.3311],\n","          ...,\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3310, 0.3310, 0.3310],\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3310, 0.3310, 0.3310],\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3310, 0.3310, 0.3311]]]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","tensor([[[[0.3217, 0.3217, 0.3217,  ..., 0.3217, 0.3217, 0.3217],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3216, 0.3216, 0.3216],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3215, 0.3215, 0.3216],\n","          ...,\n","          [0.3218, 0.3217, 0.3217,  ..., 0.3216, 0.3216, 0.3215],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3216, 0.3217, 0.3216],\n","          [0.3218, 0.3217, 0.3217,  ..., 0.3216, 0.3216, 0.3216]],\n","\n","         [[0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3472],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3474, 0.3474, 0.3474],\n","          ...,\n","          [0.3472, 0.3472, 0.3473,  ..., 0.3473, 0.3474, 0.3474],\n","          [0.3472, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3472, 0.3472, 0.3473,  ..., 0.3473, 0.3473, 0.3473]],\n","\n","         [[0.3310, 0.3310, 0.3310,  ..., 0.3311, 0.3310, 0.3311],\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3311, 0.3311, 0.3311],\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3311, 0.3311, 0.3311],\n","          ...,\n","          [0.3310, 0.3311, 0.3310,  ..., 0.3311, 0.3311, 0.3311],\n","          [0.3311, 0.3310, 0.3310,  ..., 0.3311, 0.3310, 0.3311],\n","          [0.3311, 0.3311, 0.3310,  ..., 0.3311, 0.3311, 0.3311]]]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","tensor([[[[0.3217, 0.3217, 0.3217,  ..., 0.3217, 0.3217, 0.3217],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3216, 0.3217, 0.3217],\n","          [0.3217, 0.3218, 0.3217,  ..., 0.3216, 0.3216, 0.3216],\n","          ...,\n","          [0.3216, 0.3216, 0.3217,  ..., 0.3217, 0.3216, 0.3217],\n","          [0.3217, 0.3215, 0.3217,  ..., 0.3217, 0.3217, 0.3216],\n","          [0.3217, 0.3216, 0.3216,  ..., 0.3217, 0.3217, 0.3217]],\n","\n","         [[0.3472, 0.3473, 0.3473,  ..., 0.3473, 0.3472, 0.3473],\n","          [0.3472, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3472, 0.3472, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          ...,\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3474, 0.3473,  ..., 0.3472, 0.3472, 0.3472]],\n","\n","         [[0.3310, 0.3310, 0.3310,  ..., 0.3310, 0.3311, 0.3310],\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3311, 0.3310, 0.3310],\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3311, 0.3311, 0.3310],\n","          ...,\n","          [0.3311, 0.3310, 0.3310,  ..., 0.3311, 0.3310, 0.3311],\n","          [0.3310, 0.3311, 0.3310,  ..., 0.3311, 0.3310, 0.3310],\n","          [0.3310, 0.3310, 0.3311,  ..., 0.3311, 0.3311, 0.3311]]]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","tensor([[[[0.3217, 0.3216, 0.3217,  ..., 0.3217, 0.3217, 0.3217],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3217, 0.3217, 0.3217],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3217, 0.3217, 0.3217],\n","          ...,\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3217, 0.3217, 0.3217],\n","          [0.3217, 0.3218, 0.3217,  ..., 0.3217, 0.3217, 0.3217],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3217, 0.3217, 0.3217]],\n","\n","         [[0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3472, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          ...,\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3472, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3472,  ..., 0.3473, 0.3473, 0.3473]],\n","\n","         [[0.3311, 0.3311, 0.3310,  ..., 0.3310, 0.3310, 0.3310],\n","          [0.3311, 0.3311, 0.3310,  ..., 0.3310, 0.3310, 0.3310],\n","          [0.3311, 0.3311, 0.3310,  ..., 0.3310, 0.3310, 0.3310],\n","          ...,\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3310, 0.3310, 0.3310],\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3310, 0.3310, 0.3310],\n","          [0.3310, 0.3310, 0.3311,  ..., 0.3310, 0.3310, 0.3310]]]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","tensor([[[[0.3217, 0.3217, 0.3217,  ..., 0.3217, 0.3217, 0.3217],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3217, 0.3217, 0.3217],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3217, 0.3217, 0.3217],\n","          ...,\n","          [0.3216, 0.3216, 0.3216,  ..., 0.3216, 0.3217, 0.3217],\n","          [0.3216, 0.3216, 0.3216,  ..., 0.3216, 0.3216, 0.3216],\n","          [0.3215, 0.3215, 0.3216,  ..., 0.3216, 0.3216, 0.3216]],\n","\n","         [[0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3474, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          ...,\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3474, 0.3473,  ..., 0.3473, 0.3473, 0.3473]],\n","\n","         [[0.3310, 0.3310, 0.3310,  ..., 0.3310, 0.3310, 0.3310],\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3309, 0.3310, 0.3310],\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3310, 0.3310, 0.3310],\n","          ...,\n","          [0.3311, 0.3311, 0.3311,  ..., 0.3310, 0.3310, 0.3310],\n","          [0.3311, 0.3311, 0.3311,  ..., 0.3311, 0.3310, 0.3310],\n","          [0.3311, 0.3311, 0.3311,  ..., 0.3311, 0.3311, 0.3311]]]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","tensor([[[[0.3217, 0.3217, 0.3217,  ..., 0.3216, 0.3216, 0.3216],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3217, 0.3216, 0.3216],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3216, 0.3216, 0.3216],\n","          ...,\n","          [0.3216, 0.3217, 0.3216,  ..., 0.3217, 0.3217, 0.3217],\n","          [0.3217, 0.3216, 0.3217,  ..., 0.3217, 0.3217, 0.3217],\n","          [0.3217, 0.3216, 0.3217,  ..., 0.3217, 0.3217, 0.3217]],\n","\n","         [[0.3473, 0.3473, 0.3473,  ..., 0.3475, 0.3475, 0.3475],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3475, 0.3475, 0.3475],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3475, 0.3475, 0.3475],\n","          ...,\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3474, 0.3473, 0.3473]],\n","\n","         [[0.3310, 0.3310, 0.3310,  ..., 0.3308, 0.3309, 0.3309],\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3308, 0.3308, 0.3309],\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3309, 0.3309, 0.3308],\n","          ...,\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3310, 0.3310, 0.3311],\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3310, 0.3310, 0.3310],\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3310, 0.3310, 0.3310]]]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","tensor([[[[0.3215, 0.3215, 0.3215,  ..., 0.3217, 0.3217, 0.3217],\n","          [0.3215, 0.3215, 0.3215,  ..., 0.3217, 0.3217, 0.3217],\n","          [0.3215, 0.3215, 0.3215,  ..., 0.3217, 0.3217, 0.3217],\n","          ...,\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3217, 0.3217, 0.3217],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3217, 0.3217, 0.3217],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3217, 0.3217, 0.3217]],\n","\n","         [[0.3475, 0.3474, 0.3474,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3474, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3475, 0.3474, 0.3475,  ..., 0.3473, 0.3473, 0.3473],\n","          ...,\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473]],\n","\n","         [[0.3310, 0.3311, 0.3311,  ..., 0.3311, 0.3310, 0.3310],\n","          [0.3311, 0.3312, 0.3312,  ..., 0.3310, 0.3310, 0.3310],\n","          [0.3310, 0.3311, 0.3311,  ..., 0.3310, 0.3310, 0.3310],\n","          ...,\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3310, 0.3310, 0.3310],\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3310, 0.3310, 0.3311],\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3310, 0.3310, 0.3311]]]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","tensor([[[[0.3217, 0.3217, 0.3217,  ..., 0.3217, 0.3217, 0.3217],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3216, 0.3217, 0.3217],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3216, 0.3216, 0.3216],\n","          ...,\n","          [0.3217, 0.3216, 0.3217,  ..., 0.3217, 0.3217, 0.3217],\n","          [0.3216, 0.3216, 0.3217,  ..., 0.3217, 0.3216, 0.3216],\n","          [0.3216, 0.3216, 0.3216,  ..., 0.3217, 0.3217, 0.3216]],\n","\n","         [[0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3472, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          ...,\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3474, 0.3474, 0.3474],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3474, 0.3475, 0.3474],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3475, 0.3475, 0.3475]],\n","\n","         [[0.3310, 0.3310, 0.3310,  ..., 0.3310, 0.3310, 0.3310],\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3310, 0.3310, 0.3310],\n","          [0.3310, 0.3310, 0.3311,  ..., 0.3311, 0.3310, 0.3310],\n","          ...,\n","          [0.3310, 0.3310, 0.3311,  ..., 0.3309, 0.3309, 0.3309],\n","          [0.3310, 0.3310, 0.3311,  ..., 0.3309, 0.3309, 0.3309],\n","          [0.3310, 0.3311, 0.3311,  ..., 0.3309, 0.3309, 0.3309]]]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n","tensor([[[[0.3216, 0.3217, 0.3216,  ..., 0.3215, 0.3215, 0.3216],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3216, 0.3216, 0.3216],\n","          [0.3217, 0.3216, 0.3216,  ..., 0.3216, 0.3216, 0.3216],\n","          ...,\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3217, 0.3216, 0.3217],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3216, 0.3216, 0.3216],\n","          [0.3217, 0.3217, 0.3217,  ..., 0.3216, 0.3216, 0.3216]],\n","\n","         [[0.3473, 0.3473, 0.3473,  ..., 0.3474, 0.3474, 0.3474],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3474],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3474],\n","          ...,\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473],\n","          [0.3473, 0.3473, 0.3473,  ..., 0.3473, 0.3473, 0.3473]],\n","\n","         [[0.3311, 0.3311, 0.3311,  ..., 0.3311, 0.3311, 0.3311],\n","          [0.3310, 0.3310, 0.3311,  ..., 0.3311, 0.3311, 0.3310],\n","          [0.3310, 0.3311, 0.3311,  ..., 0.3311, 0.3311, 0.3311],\n","          ...,\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3310, 0.3310, 0.3310],\n","          [0.3310, 0.3310, 0.3310,  ..., 0.3311, 0.3310, 0.3311],\n","          [0.3310, 0.3310, 0.3309,  ..., 0.3310, 0.3310, 0.3311]]]],\n","       device='cuda:0', grad_fn=<SoftmaxBackward0>)\n"]}],"source":["#def get_mask(image, threshold, loaded_net):\n","transform = transforms.ToTensor()\n","image = transform(image).unsqueeze(dim=0)\n","\n","loaded_net = UNet().to(device)\n","\n","tiles = torch.from_numpy(util.view_as_blocks(image.numpy(), (1, 1, 320, 320))).to(device)\n","final_pred = (torch.zeros((3, 4, 1, 3, 320, 320)).to(device)).type(torch.cuda.FloatTensor)\n","for (i,j) in product(range(tiles.shape[2]), range(tiles.shape[3])):\n","    tile = tiles[0, 0, i, j, ...]\n","    tile = (tile-torch.mean(tile))/torch.std(tile)\n","    tile = loaded_net(tile)\n","    print(tile)\n","    final_pred[i, j, ...] = tile\n","\n","\n","final_pred = final_pred.to('cpu')\n","#pred = util.montage(final_pred.detach().numpy(), grid_shape=(3, 4), multichannel=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9HhxHugT5Tlc"},"outputs":[],"source":["final_pred = torch.reshape(final_pred, [-1, 320, 320, 3])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1667671579127,"user":{"displayName":"Manish Sahoo","userId":"13850174497932657096"},"user_tz":-330},"id":"VbAa4lgfDGwI","outputId":"c9d48a3e-f29a-4f28-cd1f-c91d66052ef7"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([12, 320, 320, 3])"]},"metadata":{},"execution_count":21}],"source":["final_pred.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1667671580290,"user":{"displayName":"Manish Sahoo","userId":"13850174497932657096"},"user_tz":-330},"id":"4UShZgwNDP6Z","outputId":"56a6b0b1-f627-4bfa-8364-bc9ff731dbc6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[[[0.3528804 , 0.35286304, 0.35290676],\n","         [0.35286304, 0.3528804 , 0.35294712],\n","         [0.3529825 , 0.35296556, 0.35299084],\n","         ...,\n","         [0.35284674, 0.35283375, 0.3528062 ],\n","         [0.35281125, 0.35280746, 0.3528128 ],\n","         [0.3528091 , 0.3528128 , 0.35280746]],\n","\n","        [[0.35299638, 0.35294044, 0.3529974 ],\n","         [0.35294044, 0.35299638, 0.35291344],\n","         [0.35293207, 0.35300365, 0.35297483],\n","         ...,\n","         [0.35283092, 0.35282823, 0.3528084 ],\n","         [0.35281676, 0.35280758, 0.35281727],\n","         [0.3528139 , 0.35281727, 0.35280758]],\n","\n","        [[0.3529981 , 0.35296372, 0.35298946],\n","         [0.35296372, 0.3529981 , 0.35298973],\n","         [0.3530147 , 0.35304615, 0.35297316],\n","         ...,\n","         [0.3528071 , 0.35282862, 0.35281783],\n","         [0.3528216 , 0.35280496, 0.35280308],\n","         [0.35279772, 0.35280308, 0.35280496]],\n","\n","        ...,\n","\n","        [[0.3208046 , 0.32081008, 0.3208161 ],\n","         [0.32081008, 0.3208046 , 0.32080516],\n","         [0.3207914 , 0.32079735, 0.32080212],\n","         ...,\n","         [0.32083428, 0.3208403 , 0.3208499 ],\n","         [0.32084408, 0.32083696, 0.32082045],\n","         [0.32084084, 0.32082045, 0.32083696]],\n","\n","        [[0.32082045, 0.32078743, 0.32080847],\n","         [0.32078743, 0.32082045, 0.32081625],\n","         [0.32080382, 0.32080144, 0.32080644],\n","         ...,\n","         [0.32083648, 0.32082593, 0.3208408 ],\n","         [0.32081336, 0.32081115, 0.3208072 ],\n","         [0.32080308, 0.3208072 , 0.32081115]],\n","\n","        [[0.3208204 , 0.32084998, 0.32080403],\n","         [0.32084998, 0.3208204 , 0.32079166],\n","         [0.3207923 , 0.32079747, 0.3208026 ],\n","         ...,\n","         [0.3208417 , 0.3208144 , 0.3208266 ],\n","         [0.32082987, 0.32083872, 0.32082674],\n","         [0.32085064, 0.32082674, 0.32083872]]],\n","\n","\n","       [[[0.3527651 , 0.35277203, 0.35277882],\n","         [0.35277203, 0.3527651 , 0.35277292],\n","         [0.35276765, 0.35276946, 0.35274905],\n","         ...,\n","         [0.35292897, 0.3529122 , 0.3528611 ],\n","         [0.35287282, 0.35287637, 0.35289633],\n","         [0.3528802 , 0.35289633, 0.35287637]],\n","\n","        [[0.35276628, 0.3527752 , 0.35277206],\n","         [0.3527752 , 0.35276628, 0.35277203],\n","         [0.35276848, 0.35276502, 0.3527398 ],\n","         ...,\n","         [0.35296187, 0.3529344 , 0.3528763 ],\n","         [0.35292926, 0.35296848, 0.35293773],\n","         [0.3529114 , 0.35293773, 0.35296848]],\n","\n","        [[0.35275117, 0.35277307, 0.3527796 ],\n","         [0.35277307, 0.35275117, 0.35274407],\n","         [0.35276192, 0.3527585 , 0.35276008],\n","         ...,\n","         [0.35301253, 0.3529961 , 0.3530538 ],\n","         [0.3530293 , 0.35304558, 0.3530097 ],\n","         [0.35303438, 0.3530097 , 0.35304558]],\n","\n","        ...,\n","\n","        [[0.32075703, 0.32074577, 0.32077265],\n","         [0.32074577, 0.32075703, 0.32076868],\n","         [0.32072487, 0.32079944, 0.3207834 ],\n","         ...,\n","         [0.32074928, 0.3207253 , 0.32078195],\n","         [0.32075387, 0.32075477, 0.32081172],\n","         [0.32084134, 0.32081172, 0.32075477]],\n","\n","        [[0.32073203, 0.32069904, 0.32072002],\n","         [0.32069904, 0.32073203, 0.32076758],\n","         [0.32084647, 0.32081723, 0.32083064],\n","         ...,\n","         [0.32073912, 0.32071006, 0.32070926],\n","         [0.32073203, 0.3208269 , 0.32076192],\n","         [0.32079196, 0.32076192, 0.3208269 ]],\n","\n","        [[0.32077643, 0.32073155, 0.32078102],\n","         [0.32073155, 0.32077643, 0.32078886],\n","         [0.32080704, 0.3208182 , 0.3208142 ],\n","         ...,\n","         [0.32075718, 0.320747  , 0.32078108],\n","         [0.3207128 , 0.32074204, 0.32078186],\n","         [0.32083505, 0.32078186, 0.32074204]]],\n","\n","\n","       [[[0.35295486, 0.35300982, 0.35299116],\n","         [0.35300982, 0.35295486, 0.3529845 ],\n","         [0.35304493, 0.35302886, 0.35300854],\n","         ...,\n","         [0.35297307, 0.35296077, 0.35292837],\n","         [0.3529177 , 0.3529683 , 0.35299665],\n","         [0.35300067, 0.35299665, 0.3529683 ]],\n","\n","        [[0.35294628, 0.35297087, 0.35298282],\n","         [0.35297087, 0.35294628, 0.35299063],\n","         [0.35299787, 0.35302797, 0.35299426],\n","         ...,\n","         [0.35292834, 0.35288134, 0.3528967 ],\n","         [0.35294542, 0.3529278 , 0.3529957 ],\n","         [0.35293734, 0.3529957 , 0.3529278 ]],\n","\n","        [[0.35299104, 0.35301468, 0.35302758],\n","         [0.35301468, 0.35299104, 0.35288882],\n","         [0.3530346 , 0.35306972, 0.35300675],\n","         ...,\n","         [0.35291183, 0.3528959 , 0.35289538],\n","         [0.35289887, 0.35295144, 0.35295796],\n","         [0.35297275, 0.35295796, 0.35295144]],\n","\n","        ...,\n","\n","        [[0.3209063 , 0.3209425 , 0.32091498],\n","         [0.3209425 , 0.3209063 , 0.320907  ],\n","         [0.32085037, 0.3207969 , 0.32078537],\n","         ...,\n","         [0.320809  , 0.32083014, 0.32079586],\n","         [0.32081634, 0.32074383, 0.32077995],\n","         [0.32083076, 0.32077995, 0.32074383]],\n","\n","        [[0.32084113, 0.32091916, 0.32084382],\n","         [0.32091916, 0.32084113, 0.3208384 ],\n","         [0.32079172, 0.32076055, 0.32071993],\n","         ...,\n","         [0.32079202, 0.32081077, 0.32078966],\n","         [0.32079285, 0.32077435, 0.32072473],\n","         [0.32077038, 0.32072473, 0.32077435]],\n","\n","        [[0.32080975, 0.32086417, 0.32083696],\n","         [0.32086417, 0.32080975, 0.32079315],\n","         [0.32076037, 0.3207467 , 0.32074055],\n","         ...,\n","         [0.3208048 , 0.32083765, 0.3208136 ],\n","         [0.32081172, 0.32077965, 0.32079372],\n","         [0.32079148, 0.32079372, 0.32077965]]],\n","\n","\n","       ...,\n","\n","\n","       [[[0.3527686 , 0.35274103, 0.3527606 ],\n","         [0.35274103, 0.3527686 , 0.35280144],\n","         [0.35283196, 0.35281402, 0.35278967],\n","         ...,\n","         [0.3530327 , 0.3530269 , 0.35302308],\n","         [0.35301316, 0.35299075, 0.3530251 ],\n","         [0.35300174, 0.3530251 , 0.35299075]],\n","\n","        [[0.35272074, 0.35271728, 0.35272756],\n","         [0.35271728, 0.35272074, 0.3527399 ],\n","         [0.35273576, 0.35267818, 0.35275108],\n","         ...,\n","         [0.35299984, 0.35302034, 0.35302663],\n","         [0.35303837, 0.3530207 , 0.35301507],\n","         [0.3529961 , 0.35301507, 0.3530207 ]],\n","\n","        [[0.3528769 , 0.35287315, 0.3528636 ],\n","         [0.35287315, 0.3528769 , 0.3528884 ],\n","         [0.35286734, 0.35286433, 0.3528216 ],\n","         ...,\n","         [0.35304067, 0.35303015, 0.35299465],\n","         [0.3529747 , 0.3529433 , 0.35298318],\n","         [0.35293397, 0.35298318, 0.3529433 ]],\n","\n","        ...,\n","\n","        [[0.32082102, 0.32083791, 0.3208051 ],\n","         [0.32083791, 0.32082102, 0.32078582],\n","         [0.32078207, 0.3207719 , 0.32079533],\n","         ...,\n","         [0.3208283 , 0.320829  , 0.32080868],\n","         [0.32078782, 0.32082582, 0.32080966],\n","         [0.32079807, 0.32080966, 0.32082582]],\n","\n","        [[0.3208218 , 0.32081777, 0.3208169 ],\n","         [0.32081777, 0.3208218 , 0.32082427],\n","         [0.32081392, 0.32078105, 0.32079324],\n","         ...,\n","         [0.32084134, 0.3208162 , 0.32081148],\n","         [0.3208174 , 0.32084185, 0.32081154],\n","         [0.32077917, 0.32081154, 0.32084185]],\n","\n","        [[0.32080767, 0.3208262 , 0.3208452 ],\n","         [0.3208262 , 0.32080767, 0.32081264],\n","         [0.32079762, 0.3208292 , 0.32077664],\n","         ...,\n","         [0.32082722, 0.3207923 , 0.32081145],\n","         [0.32084045, 0.32085097, 0.3207968 ],\n","         [0.3208239 , 0.3207968 , 0.32085097]]],\n","\n","\n","       [[[0.35300928, 0.352987  , 0.3530131 ],\n","         [0.352987  , 0.35300928, 0.35295567],\n","         [0.35300285, 0.35296974, 0.35299945],\n","         ...,\n","         [0.3528981 , 0.35292354, 0.35294938],\n","         [0.35292876, 0.35293618, 0.3529184 ],\n","         [0.35295776, 0.3529184 , 0.35293618]],\n","\n","        [[0.3529623 , 0.35300538, 0.3529671 ],\n","         [0.35300538, 0.3529623 , 0.353006  ],\n","         [0.3529753 , 0.3530016 , 0.35297287],\n","         ...,\n","         [0.35295734, 0.3529814 , 0.3530069 ],\n","         [0.352957  , 0.35297048, 0.35297924],\n","         [0.35301578, 0.35297924, 0.35297048]],\n","\n","        [[0.35303098, 0.352998  , 0.35301623],\n","         [0.352998  , 0.35303098, 0.35298982],\n","         [0.3530211 , 0.3530031 , 0.35297856],\n","         ...,\n","         [0.3529874 , 0.35301518, 0.35300878],\n","         [0.3530269 , 0.35301882, 0.35305154],\n","         [0.35301375, 0.35305154, 0.35301882]],\n","\n","        ...,\n","\n","        [[0.32083747, 0.32084003, 0.32083464],\n","         [0.32084003, 0.32083747, 0.32083482],\n","         [0.32082307, 0.32082722, 0.32082212],\n","         ...,\n","         [0.32075053, 0.3207534 , 0.32074472],\n","         [0.32074592, 0.3207404 , 0.3207471 ],\n","         [0.32076585, 0.3207471 , 0.3207404 ]],\n","\n","        [[0.32082042, 0.3208132 , 0.3208176 ],\n","         [0.3208132 , 0.32082042, 0.32081878],\n","         [0.3208288 , 0.32082558, 0.3208294 ],\n","         ...,\n","         [0.3207543 , 0.32073855, 0.3207627 ],\n","         [0.3207776 , 0.32077044, 0.32077286],\n","         [0.32079503, 0.32077286, 0.32077044]],\n","\n","        [[0.32080814, 0.32082257, 0.32082763],\n","         [0.32082257, 0.32080814, 0.3208229 ],\n","         [0.32082063, 0.3208113 , 0.3208271 ],\n","         ...,\n","         [0.3207545 , 0.32076463, 0.32075012],\n","         [0.3207383 , 0.3207353 , 0.32075638],\n","         [0.32075465, 0.32075638, 0.3207353 ]]],\n","\n","\n","       [[[0.35298267, 0.35299754, 0.35301098],\n","         [0.35299754, 0.35298267, 0.35303265],\n","         [0.35297614, 0.35294604, 0.353017  ],\n","         ...,\n","         [0.35282734, 0.35286158, 0.3528625 ],\n","         [0.35287917, 0.35288823, 0.35292363],\n","         [0.35291952, 0.35292363, 0.35288823]],\n","\n","        [[0.35297075, 0.35299927, 0.35300422],\n","         [0.35299927, 0.35297075, 0.3530115 ],\n","         [0.35296026, 0.3529326 , 0.35300496],\n","         ...,\n","         [0.35281113, 0.3528528 , 0.35279498],\n","         [0.35277647, 0.35281238, 0.35282016],\n","         [0.35285962, 0.35282016, 0.35281238]],\n","\n","        [[0.3529566 , 0.3529814 , 0.35300034],\n","         [0.3529814 , 0.3529566 , 0.35299066],\n","         [0.35297972, 0.35297838, 0.3529915 ],\n","         ...,\n","         [0.3528642 , 0.3528845 , 0.35288358],\n","         [0.35285398, 0.35281187, 0.3528478 ],\n","         [0.35285974, 0.3528478 , 0.35281187]],\n","\n","        ...,\n","\n","        [[0.32083738, 0.32078892, 0.32083094],\n","         [0.32078892, 0.32083738, 0.32081917],\n","         [0.32083398, 0.3207803 , 0.32080027],\n","         ...,\n","         [0.32074332, 0.32077363, 0.32080796],\n","         [0.32074034, 0.32077405, 0.3208124 ],\n","         [0.32084012, 0.3208124 , 0.32077405]],\n","\n","        [[0.3208135 , 0.32080144, 0.32080045],\n","         [0.32080144, 0.3208135 , 0.32078534],\n","         [0.3208114 , 0.32076502, 0.32077584],\n","         ...,\n","         [0.3207322 , 0.32078895, 0.32078037],\n","         [0.32079056, 0.32076928, 0.32079542],\n","         [0.32080662, 0.32079542, 0.32076928]],\n","\n","        [[0.32080942, 0.32081166, 0.32078576],\n","         [0.32081166, 0.32080942, 0.3208375 ],\n","         [0.32084355, 0.32079422, 0.32078308],\n","         ...,\n","         [0.3207551 , 0.32076362, 0.3207767 ],\n","         [0.32078683, 0.3207971 , 0.32082742],\n","         [0.32080233, 0.32082742, 0.3207971 ]]]], dtype=float32)"]},"metadata":{},"execution_count":22}],"source":["final_pred.detach().numpy()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ll7-6B2zFMWB"},"outputs":[],"source":["final_pred = util.montage(final_pred.detach().numpy(), grid_shape=(3, 4), multichannel=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1667671603811,"user":{"displayName":"Manish Sahoo","userId":"13850174497932657096"},"user_tz":-330},"id":"gxFh20E0Da-a","outputId":"084128a3-9dc5-42ae-caf0-34d2b0c39a64"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(960, 1280, 3)"]},"metadata":{},"execution_count":24}],"source":["final_pred.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y1C7MvTblI8n"},"outputs":[],"source":["final_pred = np.argmax(final_pred, axis=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1667671608252,"user":{"displayName":"Manish Sahoo","userId":"13850174497932657096"},"user_tz":-330},"id":"_zGgpiazFwCh","outputId":"76cdb3e9-f5d5-4078-fd97-1f86333a4b90"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[2, 2, 2, ..., 1, 2, 1],\n","       [2, 1, 1, ..., 2, 0, 0],\n","       [0, 1, 1, ..., 1, 2, 1],\n","       ...,\n","       [2, 2, 0, ..., 2, 2, 0],\n","       [0, 1, 1, ..., 1, 2, 0],\n","       [0, 1, 1, ..., 2, 2, 1]])"]},"metadata":{},"execution_count":26}],"source":["final_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wSRt674ckSTl"},"outputs":[],"source":["image = image.numpy()[0][0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dbmxMOfSF6mC"},"outputs":[],"source":["label = np.zeros((960, 1280, 3), dtype=np.int32)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DXvUKw4eF99P"},"outputs":[],"source":["pos0 = np.where(final_pred==0)\n","pos1 = np.where(final_pred==1)\n","pos2 = np.where(final_pred==2)\n","\n","label[pos0[0], pos0[1], 0] = 255\n","label[pos1[0], pos1[1], 1] = 255\n","label[pos2[0], pos2[1], 2] = 255"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sb7aRheSF-BQ"},"outputs":[],"source":["lab_dir = '/content/drive/MyDrive/BTP/Results- Part-1/dataset/masks'\n","label = io.imread(os.path.join(lab_dir, 'mask1.png'))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1667671618379,"user":{"displayName":"Manish Sahoo","userId":"13850174497932657096"},"user_tz":-330},"id":"d3BtTiqQF-Eq","outputId":"a395cc62-9c48-4e48-cdd2-c5c77ce4ee26"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 0, 0, 0],\n","       [1, 1, 1, ..., 2, 2, 0],\n","       ...,\n","       [0, 0, 0, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0],\n","       [0, 0, 0, ..., 0, 0, 0]], dtype=uint8)"]},"metadata":{},"execution_count":31}],"source":["label"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1667671621943,"user":{"displayName":"Manish Sahoo","userId":"13850174497932657096"},"user_tz":-330},"id":"hq51iNQP0XxZ","outputId":"1a430bcc-fdb3-4cb5-eca2-e7d543dfe095"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[2, 2, 2, ..., 1, 2, 1],\n","       [2, 1, 1, ..., 2, 0, 0],\n","       [0, 1, 1, ..., 1, 2, 1],\n","       ...,\n","       [2, 2, 0, ..., 2, 2, 0],\n","       [0, 1, 1, ..., 1, 2, 0],\n","       [0, 1, 1, ..., 2, 2, 1]])"]},"metadata":{},"execution_count":32}],"source":["final_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0N9dQcQOy1_i"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iIuVio6ezg8w"},"outputs":[],"source":["#label = np.argmax(label, axis=2)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9cb9UyUR0126"},"outputs":[],"source":["#label.reshape(3, 960, 1280)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"C6qre7Rq3ESr"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}